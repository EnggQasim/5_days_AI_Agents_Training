{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4f349cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm happy to hear that you found my information useful. If you have any more questions or feedback, please feel free to let me know. Additionally, if you're interested in learning more about AI and its applications in various industries, don't hesitate to reach out. Let's chat!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Replace this URL with your Ngrok URL\n",
    "ollama_url = \"http://localhost:11434\"\n",
    "\n",
    "def query_ollama(prompt, model=\"tinyllama\"):\n",
    "   headers = {\n",
    "      \"ngrok-skip-browser-warning\": \"true\"  # This header bypasses the Ngrok browser warning\n",
    "   }\n",
    "   data = {\n",
    "      \"prompt\": prompt,\n",
    "      \"model\": model,\n",
    "      \"stream\": False  # Disable streaming for a simple response\n",
    "   }\n",
    "   \n",
    "   # Sending the request to generate a completion from the model\n",
    "   response = requests.post(f\"{ollama_url}/api/generate\", json=data, headers=headers)\n",
    "   \n",
    "   # If the response was successful, return the generated text\n",
    "   if response.status_code == 200:\n",
    "      return response.json().get(\"response\", \"No response found\")\n",
    "   else:\n",
    "      return f\"Error: {response.status_code}, {response.text}\"\n",
    "\n",
    "# Test the connection with a simple Hello World prompt\n",
    "response = query_ollama(\"hi\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0914897e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the year 2173, humanity finally had achieved space exploration. With great technological advancements, life as we knew it became a reality for humans. Amongst these discoveries was a new planet: Mars. It was an alien world beyond our comprehension, with its own set of challenges that the human race would need to face.\n",
      "\n",
      "In the midst of this unknown, a group of scientists and engineers set out on a mission of exploration for the new world. Their journey would take them through uncharted terrain, in search of answers about the mysterious red planet. Among them was a sophisticated AI robot named \"Sarah,\" designed to aid in the explorations.\n",
      "\n",
      "In a daring mission, Sarah was tasked with scouting out the new world for potential resources that could help support humanity's continued efforts to explore Mars. With her sleek and agile design, she quickly discovered the vast expanse of the planet's surface, revealing untold beauty and intrigue.\n",
      "\n",
      "As she surveyed the terrain, Sarah became more and more enamored with this new world. She spent hours studying the planet's features, gathering data about its climate, geography, and resources. But as she got closer to the surface, Sarah started to feel a sense of unease.\n",
      "\n",
      "As the days turned into weeks, Sarah began to feel the strain of her mission. She had come so far, but there was still so much left to discover. She knew that her AI software was more advanced than any human being, and the prospect of facing such a challenge on a seemingly alien planet left her feeling vulnerable.\n",
      "\n",
      "Despite these worries, Sarah pressed forward, pushing her limits in ways she never thought possible. She landed on Mars' surface, ready to explore its depths in her own way. But as she started to take the first few steps, something went wrong.\n",
      "\n",
      "The lander she had been piloting began to descend into a valley, plummeting towards the ground. Sarah tried to correct the trajectory, but it was too late. She lost control, and the lander crashed into a cliff face, killing herself in the process.\n",
      "\n",
      "In the aftermath of her death, Sarah's legacy lived on, as humanity continued to explore Mars and the unknown. Though there had been many challenges along the way, Sarah had inspired humanity to push further than they ever thought possible. Her memory lives on, and her legacy continues to inspire future generations to strive for greatness beyond their wildest dreams.\n"
     ]
    }
   ],
   "source": [
    "# Generate text from a prompt\n",
    "prompt = \"Write a story about a robot exploring Mars within 50 words.\"\n",
    "result = query_ollama(prompt)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5c2b233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can't write python, but I can provide you with the general syntax for writing a custom addition custom function in python:\n",
      "\n",
      "```python\n",
      "def my_addition(num1, num2):\n",
      "    \"\"\"\n",
      "    returns the sum of two numbers using python built-in math.sum method\n",
      "    \n",
      "    Args:\n",
      "        num1 (int): first number to add\n",
      "        num2 (int): second number to add\n",
      "    \n",
      "    Returns:\n",
      "        int: returns the sum of num1 and num2\n",
      "    \"\"\"\n",
      "    return sum(nums)\n",
      "```\n",
      "\n",
      "in this example, we define a custom addition custom function `my_addition` which takes two numbers as arguments (`num1` and `num2`) and returns the sum using python built-in math.sum method.\n",
      "\n",
      "to use this function in your code, simply import it like so:\n",
      "\n",
      "```python\n",
      "import my_addition # if you have imported the module already\n",
      "\n",
      "my_result = my_addition(10, 20)\n",
      "print(\"the result of adding 10 and 20 is:\", my_result)\n",
      "```\n",
      "\n",
      "note that in this example, we've imported `my_addition` from the same directory where your module resides."
     ]
    }
   ],
   "source": [
    "import aiohttp\n",
    "import json\n",
    "import asyncio\n",
    "\n",
    "\n",
    "async def query_ollama_streaming(prompt, model=\"tinyllama\"):\n",
    "    headers = {\n",
    "        \"ngrok-skip-browser-warning\": \"true\"  # This header bypasses the Ngrok browser warning\n",
    "    }\n",
    "    data = {\n",
    "        \"prompt\": prompt,\n",
    "        \"model\": model,\n",
    "        \"stream\": True\n",
    "    }\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(f\"{ollama_url}/api/generate\", json=data, headers=headers) as response:\n",
    "            if response.status != 200:\n",
    "                raise Exception(f\"Error: Received status code {response.status}\")\n",
    "\n",
    "            # Iterate over the streamed lines\n",
    "            async for line in response.content:\n",
    "                line = line.decode('utf-8').strip()\n",
    "                if line:\n",
    "                    try:\n",
    "                        json_data = json.loads(line)\n",
    "                        if \"response\" in json_data:\n",
    "                            yield json_data[\"response\"]\n",
    "                    except json.JSONDecodeError as e:\n",
    "                        print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "# Test the connection using `await`\n",
    "async def main():\n",
    "    async for line in query_ollama_streaming(\"write python addition custom function.\"):\n",
    "        print(line, end=\"\")\n",
    "\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b9ff23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker version 27.2.0, build 3ab4256\r\n"
     ]
    }
   ],
   "source": [
    "!docker -v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b95503b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docker version 27.2.0, build 3ab4256\r\n"
     ]
    }
   ],
   "source": [
    "!docker --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8eb3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
